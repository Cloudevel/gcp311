{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![cloudevel](img/cloudevel.png)](https://www.cloudevel.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow y Composer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow es una plataforma de código abierto diseñada para crear, programar, ejecutar y monitorear flujos de trabajo (workflows). Es una herramienta muy popular en el ámbito de la ingeniería de datos, utilizada para la orquestación de pipelines de datos complejos.\n",
    "\n",
    "**Características**\n",
    "\n",
    "* **Flujos de trabajo como código:** En Airflow, los flujos de trabajo se definen como código Python. Esto ofrece flexibilidad, permite reutilización, y facilita prácticas como la integración con sistemas de control de versiones (ej.: Git).\n",
    "* **Orientado a DAGs:** Airflow utiliza el concepto de Grafos Acíclicos Dirigidos (DAGs en inglés). Un DAG es una estructura que representa una colección de tareas a ejecutar y sus dependencias; garantizando un orden de ejecución específico.\n",
    "* **Extensible:**  Airflow se integra con una amplia variedad de tecnologías y servicios externos como bases de datos, servicios en la nube (AWS, Google Cloud, Azure), herramientas de aprendizaje automático (Machine Learning) y más.\n",
    "* **Interfaz de usuario amigable:** Cuenta con una interfaz web que permite visualizar el estado de tus workflows, dependencias entre tareas, registros de ejecución, y gestionar tus procesos.\n",
    "* **Escalable:** Airflow puede escalarse para manejar grandes volúmenes de datos y tareas complejas.\n",
    "\n",
    "**¿Por qué usar Apache Airflow?**\n",
    "\n",
    "* **Orquestación de pipelines de datos:**  Si trabajas con pipelines que involucran múltiples pasos, dependencias de tareas, y fuentes de datos heterogéneas, Airflow te brinda un control centralizado.\n",
    "* **Automatización de procesos:** Puedes programar tareas repetitivas (por ejemplo: extracciones diarias de datos, reportes semanales, entrenamiento de modelos) para que se ejecuten de forma automática.\n",
    "* **Monitoreo y visibilidad:** Airflow ofrece herramientas para monitorear el estado de tus flujos de trabajo, facilitando la identificación de errores o cuellos de botella.\n",
    "* **Colaboración:** Al utilizar definiciones de flujos de trabajos escritas en Python, los equipos pueden colaborar más fácilmente en la construcción y mantenimiento de estos pipelines.\n",
    "\n",
    "\n",
    "* **Sitio web oficial:** [https://airflow.apache.org/](https://airflow.apache.org/) \n",
    "* **Documentación:** [https://airflow.apache.org/docs/apache-airflow/stable/index.html](https://airflow.apache.org/docs/apache-airflow/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La construcción de DAGs (Directed Acyclic Graphs) en Apache Airflow implica definir un conjunto de tareas y sus dependencias, utilizando código Python. El proceso básico involucra los siguientes pasos:\n",
    "\n",
    "**1. Importar librerías:**\n",
    "\n",
    "Comienza importando las librerías necesarias de Airflow, incluyendo `DAG`, `from_dags`, `Operators` y otras relevantes para las tareas que se realizarán.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.providers.google.cloud.operators.bigquery_operator import BigQueryOperator\n",
    "# Librerías adicionales según las tareas\n",
    "```\n",
    "\n",
    "**2. Definir el DAG:**\n",
    "\n",
    "Crea una instancia de la clase `DAG` especificando un identificador único, una descripción, un intervalo de ejecución (por ejemplo, diario, semanal) y otras configuraciones.\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'depends_on_past': False,\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "\n",
    "dag = DAG('mi_dag',\n",
    "          default_args=default_args,\n",
    "          description='Mi primer DAG en Airflow',\n",
    "          schedule_interval='@daily')\n",
    "```\n",
    "\n",
    "**3. Crear tareas:**\n",
    "\n",
    "Define las tareas que conforman el DAG. Cada tarea se representa como una instancia de una clase de operador de Airflow, como `PythonOperator`, `BigQueryOperator`, `BashOperator`, etc.\n",
    "\n",
    "```python\n",
    "def extraer_datos():\n",
    "    # Extraer datos de la fuente (por ejemplo, API, base de datos)\n",
    "    # Procesar y transformar los datos\n",
    "    # Guardar los datos en un formato temporal\n",
    "\n",
    "t_extraer_datos = PythonOperator(\n",
    "    task_id='extraer_datos',\n",
    "    python_callable=extraer_datos,\n",
    "    dag=dag)\n",
    "\n",
    "def cargar_datos_bigquery():\n",
    "    # Cargar los datos procesados en BigQuery\n",
    "    pass\n",
    "\n",
    "t_cargar_datos_bigquery = BigQueryOperator(\n",
    "    task_id='cargar_datos_bigquery',\n",
    "    sql='INSERT INTO my_dataset.my_table (column1, column2) VALUES (?, ?)',\n",
    "    # Opciones adicionales de BigQueryOperator\n",
    "    dag=dag)\n",
    "```\n",
    "\n",
    "**4. Definir dependencias entre tareas:**\n",
    "\n",
    "Establece las dependencias entre las tareas usando la función `>>` o argumentos como `depends_on_past`. Esto indica el orden en que se deben ejecutar las tareas.\n",
    "\n",
    "```python\n",
    "t_extraer_datos >> t_cargar_datos_bigquery\n",
    "```\n",
    "\n",
    "**5. Completar el DAG:**\n",
    "\n",
    "Agrega cualquier tarea adicional y configuraciones necesarias para completar el flujo de trabajo.\n",
    "\n",
    "**6. Guardar el archivo DAG:**\n",
    "\n",
    "Guarda el código Python en un archivo con extensión `.py` dentro de la carpeta `dags` de tu instalación de Airflow.\n",
    "\n",
    "**Ejemplo completo:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.providers.google.cloud.operators.bigquery_operator import BigQueryOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'depends_on_past': False,\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "\n",
    "dag = DAG('mi_dag',\n",
    "          default_args=default_args,\n",
    "          description='Mi primer DAG en Airflow',\n",
    "          schedule_interval='@daily')\n",
    "\n",
    "def extraer_datos():\n",
    "    # Extraer datos de la fuente (por ejemplo, API, base de datos)\n",
    "    # Procesar y transformar los datos\n",
    "    # Guardar los datos en un formato temporal\n",
    "\n",
    "t_extraer_datos = PythonOperator(\n",
    "    task_id='extraer_datos',\n",
    "    python_callable=extraer_datos,\n",
    "    dag=dag)\n",
    "\n",
    "def cargar_datos_bigquery():\n",
    "    # Cargar los datos procesados en BigQuery\n",
    "    pass\n",
    "\n",
    "t_cargar_datos_bigquery = BigQueryOperator(\n",
    "    task_id='cargar_datos_bigquery',\n",
    "    sql='INSERT INTO my_dataset.my_table (column1, column2) VALUES (?, ?)',\n",
    "    # Opciones adicionales de BigQueryOperator\n",
    "    dag=dag)\n",
    "\n",
    "t_extraer_datos >> t_cargar_datos_bigquery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Servicio gestionado de Apache Airflow:** Cloud Composer es un servicio totalmente gestionado dentro de Google Cloud Platform (GCP) que se basa en la popular plataforma de código abierto Apache Airflow. \n",
    "* **Simplifica la creación y administración de flujos de trabajo:**  Cloud Composer se encarga de la infraestructura, configuración y actualizaciones de Airflow, permitiéndote enfocarte en la definición de tus procesos y flujos de trabajo.\n",
    "* **Orientado a procesos ETL, analítica y aprendizaje automático:** Está optimizado para flujos de trabajo relacionados con extracción, transformación y carga de datos (ETL), analítica de datos y procesos de machine learning. \n",
    "\n",
    "**Características principales**\n",
    "\n",
    "* **Integración nativa con Google Cloud:** Composer se conecta fácilmente con otros servicios de Google Cloud como BigQuery, Cloud Storage, Pub/Sub, etc. simplificando tareas comunes en la nube.\n",
    "* **Escalabilidad:** Al estar basado en GCP, Composer puede escalar tus flujos de trabajo automáticamente para adaptarse a cambios en la carga de trabajo.\n",
    "* **Seguridad:** Composer cumple con estándares de seguridad y privacidad, proporcionando tranquilidad para cargas de trabajo sensibles.\n",
    "* **Monitoring sencillo:** La interfaz integrada de Airflow y las herramientas de monitoreo de GCP ayudan a visualizar los flujos de trabajo y a solucionar problemas.\n",
    "* **Entornos de Airflow actualizados:** Composer te mantiene actualizado con las últimas versiones de Airflow, proporcionando acceso a mejoras y nuevas características.\n",
    "\n",
    "Guías y tutoriales de Cloud Composer en la documentación oficial: [https://cloud.google.com/composer/docs/](https://cloud.google.com/composer/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2024.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
