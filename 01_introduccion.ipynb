{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![cloudevel](img/cloudevel.png)](https://www.cloudevel.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptos básicos  de Ingeniería de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **Ingeniería de datos** es una especialidad que se enfoca en el diseño, construcción y mantenimiento de sistemas para la recopilación, almacenamiento, transformación y análisis de grandes volúmenes de datos. Los ingenieros de datos trabajan con una variedad de tecnologías y herramientas para crear infraestructuras de datos escalables y confiables que permitan a las organizaciones obtener información valiosa de sus datos.\n",
    "\n",
    "**Las responsabilidades principales de un ingeniero de datos incluyen:**\n",
    "\n",
    "* **Recopilación y extracción de datos:** Recopilar datos de diversas fuentes, como bases de datos, archivos web y sensores de IoT.\n",
    "* **Limpieza y transformación de datos:** Limpiar y transformar los datos brutos para eliminar errores, inconsistencias y valores atípicos.\n",
    "* **Almacenamiento de datos:** Diseñar e implementar sistemas para almacenar grandes volúmenes de datos de manera eficiente y segura.\n",
    "* **Procesamiento de datos:** Procesar y transformar los datos almacenados para prepararlos para el análisis.\n",
    "* **Análisis de datos:** Colaborar con científicos de datos y analistas de negocios para analizar los datos y extraer información útil.\n",
    "* **Construcción de *pipelines* de datos:** Diseñar y construir *pipelines* de datos para automatizar el flujo de datos entre diferentes sistemas.\n",
    "* **Monitoreo y mantenimiento de datos:** Monitorear el rendimiento de los sistemas de datos y solucionar problemas cuando sea necesario.\n",
    "\n",
    "**Las habilidades clave para un ingeniero de datos incluyen:**\n",
    "\n",
    "* **Habilidades de programación:** Conocimiento de lenguajes de programación como Python, Java y SQL.\n",
    "* **Habilidades en bases de datos:** Conocimiento de sistemas de bases de datos relacionales y no relacionales.\n",
    "* **Habilidades en la nube:** Conocimiento de plataformas de computación en la nube como AWS, Azure y GCP.\n",
    "* **Habilidades de big data:** Conocimiento de herramientas y tecnologías de big data como Hadoop, Spark y Kafka.\n",
    "* **Habilidades de análisis de datos:** Conocimiento de estadísticas, [aprendizaje automático](https://www.statlearning.com/) y [minería de datos](https://dataminingbook.info/book/).\n",
    "* **Habilidades de comunicación:** Capacidad para comunicarse de manera efectiva con técnicos y no técnicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "https://www.bbva.com/es/innovacion/las-cinco-uves-del-big-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **pipeline de datos** (o tubería de datos) es una serie automatizada de pasos o procesos que se utilizan para recopilar, transformar y mover datos de una fuente a un destino. Estos pipelines son esenciales para las organizaciones que manejan grandes volúmenes de datos, ya que permiten procesar y analizar los datos de manera eficiente y escalable.\n",
    "\n",
    "**Los pipelines de datos se utilizan comúnmente para:**\n",
    "\n",
    "* **Extraer datos de diversas fuentes:** Esto puede incluir bases de datos, archivos web, sensores de IoT y redes sociales.\n",
    "* **Limpieza y transformación de datos:** Eliminar errores, inconsistencias y valores atípicos de los datos.\n",
    "* **Transformar los datos en un formato adecuado para el análisis:** Esto puede implicar agregar, filtrar, unir y derivar nuevas características de los datos.\n",
    "* **Cargar los datos en un almacén de datos o lago de datos:** Estos son repositorios centrales donde se almacenan los datos para su análisis.\n",
    "* **Realizar análisis de datos:** Los datos limpios y transformados se pueden utilizar para realizar análisis estadísticos, aprendizaje automático y minería de datos.\n",
    "* **Monitorear y mantener el pipeline:** Es importante monitorear el rendimiento del pipeline para asegurarse de que está funcionando correctamente y realizar ajustes según sea necesario.\n",
    "\n",
    "**Los beneficios de usar pipelines de datos incluyen:**\n",
    "\n",
    "* **Mayor eficiencia:** Los pipelines de datos automatizan tareas repetitivas, lo que libera tiempo para que los analistas de datos se centren en tareas más complejas.\n",
    "* **Mejor precisión:** Los pipelines de datos pueden ayudar a reducir errores y mejorar la precisión de los datos.\n",
    "* **Escalabilidad:** Los pipelines de datos se pueden escalar para manejar grandes volúmenes de datos.\n",
    "* **Mejora en la toma de decisiones:** Los datos limpios, precisos y actualizados pueden conducir a una mejor toma de decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagramas de pipeline de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagrama de  dominios.\n",
    "\n",
    "En un diagrama común de ingeniería de datos, los dominios principales que se definen typically son:\n",
    "\n",
    "**1. Fuentes de datos:**\n",
    "\n",
    "* **Tipos de datos:** Especificar el origen y el tipo de datos que se ingieren, como bases de datos relacionales, archivos CSV, registros de servidores, feeds de API y datos en tiempo real.\n",
    "* **Ubicación de los datos:** Indicar la ubicación física o lógica de los datos, ya sea en servidores locales, en la nube o en sistemas híbridos.\n",
    "* **Frecuencia de actualización:** Establecer la frecuencia con la que se actualizan los datos, como en tiempo real, por lotes o a intervalos específicos.\n",
    "* **Formatos de datos:** Describir los formatos de archivo o estructuras de datos utilizadas, como JSON, XML, CSV o formatos específicos de la industria.\n",
    "\n",
    "**2. Almacenamiento de datos:**\n",
    "\n",
    "* **Tipo de almacenamiento:** Elegir el tipo de almacenamiento de datos adecuado, como bases de datos relacionales, almacenes de datos NoSQL, lagos de datos o data warehouses en la nube.\n",
    "* **Arquitectura de almacenamiento:** Diseñar la arquitectura de almacenamiento para optimizar el rendimiento, la escalabilidad y la seguridad.\n",
    "* **Herramientas de almacenamiento:** Seleccionar las herramientas de almacenamiento adecuadas, como Apache Hadoop, Amazon S3 o Microsoft Azure Blob Storage.\n",
    "* **Políticas de almacenamiento:** Implementar políticas de almacenamiento para la retención de datos, la replicación y la gobernanza.\n",
    "\n",
    "**3. Procesamiento de datos:**\n",
    "\n",
    "* **Limpieza de datos:** Identificar y corregir errores, inconsistencias y valores atípicos en los datos.\n",
    "* **Transformación de datos:** Transformar los datos brutos en un formato adecuado para el análisis, como agregar, filtrar, unir y derivar nuevas características.\n",
    "* **Enriquecimiento de datos:** Combinar datos de diversas fuentes para mejorar su calidad y valor.\n",
    "* **Normalización de datos:** Estandarizar los formatos de datos y las convenciones de nomenclatura para facilitar la integración y el análisis.\n",
    "\n",
    "**4. Análisis de datos:**\n",
    "\n",
    "* **Entorno de análisis:** Seleccionar el entorno de análisis adecuado, como herramientas de ciencia de datos basadas en Python o R, plataformas de análisis visual o cuadernos de Jupyter.\n",
    "* **Técnicas de análisis:** Aplicar técnicas de análisis de datos adecuadas, como análisis estadístico, aprendizaje automático, minería de datos y visualización de datos.\n",
    "* **Modelos de análisis:** Desarrollar modelos de análisis para predecir tendencias, identificar patrones y extraer información útil de los datos.\n",
    "* **Comunicación de resultados:** Comunicar los resultados del análisis de manera efectiva a las partes interesadas, utilizando paneles de control, informes y presentaciones.\n",
    "\n",
    "**5. Monitoreo y mantenimiento:**\n",
    "\n",
    "* **Monitoreo del rendimiento:** Monitorear el rendimiento de los sistemas de datos para identificar y solucionar problemas de manera proactiva.\n",
    "* **Mantenimiento de la infraestructura:** Realizar tareas de mantenimiento preventivo y correctivo en la infraestructura de datos.\n",
    "* **Gestión de seguridad:** Implementar medidas de seguridad para proteger los datos y la infraestructura de amenazas cibernéticas.\n",
    "* **Gobernanza de datos:** Establecer políticas y procedimientos para garantizar la calidad, la integridad y la seguridad de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/01/plantilla_pipeline.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG.\n",
    "En ingeniería de datos, un **DAG** (Directed Acyclic Graph o Grafo Acíclico Dirigido) representa la estructura y el flujo de datos dentro de un **pipeline de datos**. A diferencia de un diagrama de flujo tradicional, que puede tener bucles y ciclos, un DAG asegura que las tareas se ejecuten en un orden específico sin la posibilidad de dependencias circulares. \n",
    "\n",
    "**Las características principales de un DAG en ingeniería de datos son:**\n",
    "\n",
    "* **Nodos:** Cada nodo en el DAG representa una tarea específica dentro del pipeline. Las tareas pueden incluir la extracción de datos, la limpieza de datos, la transformación de datos, el análisis de datos y la carga de datos.\n",
    "* **Aristas:** Las aristas conectan los nodos y representan la dependencia entre las tareas. Una arista dirigida desde un nodo A a un nodo B indica que la tarea B no puede comenzar hasta que la tarea A haya finalizado.\n",
    "* **Flujo de datos:** El flujo de datos a través del DAG se determina por la dirección de las aristas. Los datos fluyen desde los nodos de origen (nodos sin entradas) a través de los nodos intermedios hasta los nodos de destino (nodos sin salidas).\n",
    "* **Ausencia de ciclos:** Un DAG es acíclico, lo que significa que no hay rutas que formen bucles o ciclos cerrados. Esto asegura que las tareas no se ejecuten indefinidamente o se bloqueen entre sí.\n",
    "\n",
    "**Los beneficios de usar un DAG en ingeniería de datos incluyen:**\n",
    "\n",
    "* **Claridad y visibilidad:** Un DAG proporciona una representación visual clara del flujo de datos y las dependencias entre las tareas, lo que facilita la comprensión y el mantenimiento del pipeline.\n",
    "* **Ejecución eficiente:** El orden de las tareas en un DAG está optimizado para evitar dependencias circulares y garantizar una ejecución eficiente del pipeline.\n",
    "* **Escalabilidad:** Los DAGs se pueden escalar fácilmente para manejar pipelines de datos complejos con múltiples tareas y dependencias.\n",
    "* **Facilidad de depuración:** La estructura acíclica de un DAG facilita la identificación y resolución de problemas en el pipeline.\n",
    "\n",
    "**Herramientas comunes para crear y ejecutar DAGs en ingeniería de datos incluyen:**\n",
    "\n",
    "* **Apache Airflow:** Una plataforma de código abierto popular para la orquestación de flujos de trabajo de datos que utiliza DAGs para definir la estructura y el flujo de las tareas.\n",
    "* **Luigi:** Una biblioteca de Python para la programación de tareas basada en DAGs.\n",
    "* **Prefect:** Otra biblioteca de Python para la orquestación de flujos de trabajo que utiliza DAGs para definir la dependencia y el orden de las tareas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/01/dag_airflow.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heramientas de GCP para elaboración de pipelines.\n",
    "\n",
    "Google Cloud Platform (GCP) ofrece varias herramientas para crear y ejecutar pipelines de datos, cada una con sus propias características y ventajas:\n",
    "\n",
    "**1. Cloud Dataflow:**\n",
    "\n",
    "* **Servicio de procesamiento de datos totalmente gestionado:** Ejecuta pipelines batch y streaming con alta escalabilidad y rendimiento.\n",
    "* **Soporta diversos lenguajes de programación:** Python, Java y Go.\n",
    "* **Integración nativa con otros servicios de GCP:** BigQuery, Pub/Sub, Cloud Storage, etc.\n",
    "* **Ideal para:** Pipelines de datos complejos, análisis a gran escala, procesamiento en tiempo real.\n",
    "\n",
    "**2. Cloud Composer:**\n",
    "\n",
    "* **Entorno de orquestación de flujos de trabajo basado en Apache Airflow:** Permite crear, programar y monitorear pipelines de datos.\n",
    "* **Interfaz web intuitiva:** Simplifica la creación y gestión de pipelines.\n",
    "* **Integración con Cloud Dataflow y otros servicios de GCP:** Facilita la ejecución de pipelines en GCP.\n",
    "* **Ideal para:** Pipelines de datos con múltiples tareas y dependencias, equipos que colaboran en el desarrollo de pipelines.\n",
    "\n",
    "**3. Cloud Dataproc:**\n",
    "\n",
    "* **Servicio de procesamiento de datos basado en Apache Spark y Hadoop:** Ejecuta pipelines batch y streaming en clusters escalables.\n",
    "* **Altamente personalizable:** Permite configurar el entorno de ejecución según las necesidades específicas.\n",
    "* **Ideal para:** Pipelines de datos que requieren un alto grado de control y personalización, análisis de big data complejos.\n",
    "\n",
    "**4. Cloud Data Fusion:**\n",
    "\n",
    "* **Servicio de integración de datos totalmente gestionado:** Facilita la creación, ejecución y administración de pipelines de datos ETL/ELT de manera sencilla y orientada a lo visual.\n",
    "* **Interfaz gráfica e intuitiva:** No requiere habilidades avanzadas de programación. Utiliza una interfaz de \"arrastrar y soltar\" y transformaciones prediseñadas.\n",
    "* **Amplia biblioteca de conectores:** Permite la conexión con diferentes fuentes de datos y destinos, incluyendo bases de datos, almacenes de datos, aplicaciones SaaS y recursos localmente.\n",
    "* **Integración nativa con otros servicios de GCP:** Interactúa fácilmente con BigQuery, Cloud Storage, Pub/Sub y otros servicios de la plataforma.\n",
    "* **Ideal para:**  Analistas de datos, ingenieros de datos, y aquellos que prefieren un enfoque visual para la creación de pipelines de datos sin código manual extenso.\n",
    "\n",
    "### Tabla comparativa de herramientas de Google Cloud Platform para análisis de datos:\n",
    "\n",
    "| Herramienta | Enfoque principal | Casos de uso | Ventajas | Desventajas |\n",
    "|---|---|---|---|---|\n",
    "| **Dataflow (Apache Beam)** | Procesamiento de datos en batch y streaming | Pipelines de datos escalables y complejos, ETL, transformaciones de datos personalizadas | Altamente escalable, flexible, procesamiento en tiempo real | Requiere conocimientos de programación |\n",
    "| **Composer (Apache Airflow)** | Orquestación de flujos de trabajo | Coordinación de tareas y dependencias entre pipelines, programación de trabajos recurrentes | Visión completa del flujo de trabajo, monitoreo centralizado | Requiere conocimientos de Python y DAGs |\n",
    "| **Data Fusion** | Creación visual de pipelines ETL/ELT | Entorno de bajo código para crear pipelines rápidamente, fácil integración con fuentes de datos | Interfaz gráfica amigable, rapidez en la creación de pipelines | Menos flexible para transformaciones complejas |\n",
    "| **Dataprep (Trifacta)** | Preparación y limpieza de datos | Limpieza, perfilado y transformación de datos antes del análisis | Interfaz intuitiva, descubrimiento automático de patrones, sugerencias inteligentes | No apto para grandes volúmenes de datos |\n",
    "| **Dataproc** | Ejecución de clústeres Spark y Hadoop | Procesamiento a gran escala, Machine Learning, análisis avanzado | Flexible, escalable, control granular de clústeres | Requiere conocimientos de Spark/Hadoop, mayor complejidad de gestión |\n",
    "\n",
    "**Consideraciones adicionales:**\n",
    "\n",
    "* La elección de la herramienta depende de las necesidades específicas de cada proyecto y del equipo.\n",
    "* Es posible combinar varias herramientas para obtener un flujo de trabajo completo.\n",
    "* Google Cloud Platform ofrece recursos gratuitos para probar estas herramientas.\n",
    "\n",
    "Espero que esta tabla comparativa te sea útil para elegir la herramienta adecuada para tu proyecto de análisis de datos en Google Cloud Platform.\n",
    "\n",
    "\n",
    "* **Comparación de herramientas de procesamiento de datos de GCP: [https://stackoverflow.com/questions/54154816/using-dataflow-vs-cloud-composer](https://stackoverflow.com/questions/54154816/using-dataflow-vs-cloud-composer)**\n",
    "* **Elegir la herramienta de procesamiento de datos adecuada en GCP: [https://www.youtube.com/watch?v=6JO8lYgCrfY](https://www.youtube.com/watch?v=6JO8lYgCrfY)**\n",
    "* **Documentación de Cloud Dataflow: [https://cloud.google.com/dataflow/docs](https://cloud.google.com/dataflow/docs)**\n",
    "* **Documentación de Cloud Composer: [https://cloud.google.com/composer/docs/concepts/overview](https://cloud.google.com/composer/docs/concepts/overview)**\n",
    "* **Documentación de Cloud Dataproc: [https://cloud.google.com/dataproc/docs](https://cloud.google.com/dataproc/docs)**\n",
    "* **Documentación de Cloud Functions: [https://cloud.google.com/functions/docs](https://cloud.google.com/functions/docs)**\n",
    "* **Documentación de Dataflow for BigQuery: [https://cloud.google.com/dataflow](https://cloud.google.com/dataflow)**\n",
    "* **Documentación de Cloud Kubeflow: [https://cloud.google.com/kubernetes-engine](https://cloud.google.com/kubernetes-engine)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2024.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
