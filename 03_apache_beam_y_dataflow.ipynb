{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![cloudevel](img/cloudevel.png)](https://www.cloudevel.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam y Cloud Dataflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam es un modelo de programación de código abierto que te permite definir y ejecutar pipelines (tuberías) de procesamiento de datos. Ofrece una abstracción que simplifica las tareas de manejar datos tanto en procesamiento por lotes (batch) como en tiempo real (streaming).\n",
    "\n",
    "Proporciona SDKs para varios lenguajes de programación, incluyendo Java, Python, Go, entre otros. Esto te da flexibilidad para trabajar con el lenguaje que prefieras.\n",
    "\n",
    "Los pipelines creados con Apache Beam se pueden ejecutar en distintos entornos de procesamiento distribuido (runners) como Apache Flink, Apache Spark, Google Cloud Dataflow, y otros. Esto significa que es posible desarrollar pipelines una vez y luego desplegarlos en la plataforma que mejor convenga.\n",
    "\n",
    "**Características**\n",
    "\n",
    "* **Batch y Streaming en Uno:** Unifica el desarrollo de pipelines para procesar datos históricos (por lotes) y datos en tiempo real (streaming) con una sola API.\n",
    "* **Extensible:** Permite crear transformaciones personalizadas e integraciones con otras tecnologías, expandiendo sus posibilidades.\n",
    "* **Escalabilidad**:  Los runners distribuidos se encargan de la ejecución paralela y a gran escala de los pipelines, haciendo que Apache Beam pueda procesar enormes cantidades de datos.\n",
    "* **Tolerancia a fallos:** Gestiona automáticamente la recuperación ante fallos de nodos o problemas durante el procesamiento, garantizando la consistencia de tus datos.\n",
    "\n",
    "**¿Por qué usar Apache Beam?**\n",
    "\n",
    "* **Simplifica el desarrollo:** Al unificar los dos paradigmas principales de procesamiento de datos, ayuda a reducir la complejidad asociada a la gestión de tecnologías por separado.\n",
    "* **Código independiente de la plataforma:** Facilita la migración a diferentes entornos de ejecución si fuese necesario.\n",
    "* **Versátil y potente:** Gracias a sus SDKs multi-lenguaje y su extensibilidad, se adapta a una amplia gama de casos de uso en procesamiento de datos.\n",
    "* **Comunidad activa:** Siendo un proyecto de Apache Software Foundation, cuenta con una gran comunidad de desarrolladores y constante actualización.\n",
    "\n",
    "[https://beam.apache.org/](https://beam.apache.org/) \n",
    "\n",
    "[https://beam.apache.org/releases/pydoc/2.55.1/](https://beam.apache.org/releases/pydoc/2.55.1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos para construir DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Los DAGs (Directed Acyclic Graphs o Gráficos Acíclicos Dirigidos) en Apache Beam se construyen utilizando principalmente dos tipos de componentes:\n",
    "\n",
    "**1. PCollections (Colecciones de canalización)**\n",
    "\n",
    "* Representan conjuntos de datos que fluyen a través de la canalización.\n",
    "* Se pueden crear a partir de diversas fuentes, como archivos, bases de datos o mensajes de cola.\n",
    "* Las PCollections son inmutables, lo que significa que no se pueden modificar después de crearse.\n",
    "\n",
    "**2. Transformaciones**\n",
    "\n",
    "* Son las operaciones que se aplican a las PCollections para procesar los datos.\n",
    "* Existen dos tipos principales de transformaciones:\n",
    "    * **Transformaciones de un solo elemento:** Procesan cada elemento de la PCollection de forma independiente.\n",
    "    * **Transformaciones de ventana:** Agrupan los elementos de la PCollection en ventanas temporales y luego aplican una transformación a cada ventana.\n",
    "\n",
    "**Componentes adicionales**\n",
    "\n",
    "Además de PCollections y transformaciones, los DAGs en Apache Beam también pueden incluir:\n",
    "\n",
    "* **Fuentes:** Leen datos de un origen externo, como un archivo o una base de datos.\n",
    "* **Receptores:** Escriben datos en un destino externo, como un archivo o una base de datos.\n",
    "* **Triggers:** Controlan cuándo se ejecutan las transformaciones.\n",
    "* **Flujos laterales:** Permiten que los datos fluyan entre diferentes partes de la canalización.\n",
    "\n",
    "**Ejemplo de un DAG**\n",
    "\n",
    "Un DAG simple que lee datos de un archivo, los filtra y luego los escribe en otro archivo podría verse así:\n",
    "\n",
    "```\n",
    "datos_sin_filtrar = (\n",
    "    ReadFromText(\"data.txt\")\n",
    "    | FlatMap(lambda linea: linea.split())\n",
    ")\n",
    "\n",
    "datos_filtrados = (\n",
    "    datos_sin_filtrar\n",
    "    | Filter(lambda palabra: len(palabra) > 5)\n",
    ")\n",
    "\n",
    "WriteToFile(datos_filtrados, \"data_filtrada.txt\")\n",
    "```\n",
    "\n",
    "En este ejemplo:\n",
    "\n",
    "* `ReadFromText` es una fuente que lee datos del archivo \"data.txt\".\n",
    "* `FlatMap` es una transformación que divide cada línea del archivo en palabras individuales.\n",
    "* `Filter` es una transformación que filtra las palabras que tienen más de 5 caracteres.\n",
    "* `WriteToFile` es un receptor que escribe los datos filtrados en el archivo \"data_filtrada.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flujos de stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Procesamiento basado en streams en Apache Beam\n",
    "\n",
    "Apache Beam ofrece un modelo de programación unificado para procesar datos tanto en **batch** (por lotes) como en **streaming**. El procesamiento en streaming permite procesar datos en tiempo real, a medida que llegan, lo que es ideal para aplicaciones como el análisis de datos en tiempo real, la detección de fraudes o la monitorización de sistemas.\n",
    "\n",
    "**Ventanas en Apache Beam**\n",
    "\n",
    "Para procesar datos en streaming, Apache Beam utiliza el concepto de **ventanas**. Las ventanas son bloques de tiempo que agrupan los elementos de un stream. Esto permite aplicar transformaciones a los datos de forma agregada, como calcular promedios, sumas o conteos.\n",
    "\n",
    "**Tipos de ventanas en Apache Beam**\n",
    "\n",
    "Apache Beam proporciona varios tipos de ventanas para diferentes casos de uso:\n",
    "\n",
    "* **Ventanas fijas:** Dividen el tiempo en intervalos de tamaño fijo, como 1 minuto, 5 minutos o 1 hora.\n",
    "* **Ventanas deslizantes:** Se superponen entre sí, lo que permite procesar los datos de forma continua.\n",
    "* **Ventanas por sesión:** Agrupan los elementos del stream en función de la actividad del usuario o de la máquina.\n",
    "* **Ventanas personalizadas:** Permiten crear ventanas personalizadas utilizando funciones lambda.\n",
    "\n",
    "**Ejemplo de procesamiento en streaming**\n",
    "\n",
    "El siguiente ejemplo muestra cómo calcular la media de los valores de temperatura cada minuto a partir de un stream de datos de sensores:\n",
    "\n",
    "```python\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  # Leer datos del stream de sensores\n",
    "  datos_sensores = (\n",
    "      pipeline\n",
    "      | \"LeerDatosSensores\" >> beam.io.ReadFromPubSub(\"topic-sensores\")\n",
    "      | \"DecodificarDatos\" >> beam.Map(lambda dato: parsear_dato_sensor(dato))\n",
    "  )\n",
    "\n",
    "  # Calcular la media de la temperatura por minuto\n",
    "  medias_temperatura = (\n",
    "      datos_sensores\n",
    "      | \"VentanasFijas\" >> beam.WindowInto(beam.window.FixedWindows(60))\n",
    "      | \"CalcularMediaTemperatura\" >> beam.CombinePerKey(beam.combiners.MeanCombine())\n",
    "  )\n",
    "\n",
    "  # Escribir las medias de temperatura en un archivo\n",
    "  medias_temperatura | \"EscribirAArchivo\" >> beam.io.WriteToText(\"medias_temperatura.txt\")\n",
    "```\n",
    "\n",
    "En este ejemplo:\n",
    "\n",
    "* `ReadFromPubSub` lee datos del stream de sensores en un tema de Pub/Sub.\n",
    "* `DecodificarDatos` decodifica los datos del sensor en un formato utilizable.\n",
    "* `FixedWindows` divide el stream en ventanas de un minuto.\n",
    "* `CalcularMediaTemperatura` calcula la media de la temperatura para cada ventana.\n",
    "* `WriteToText` escribe las medias de temperatura en un archivo.\n",
    "\n",
    "**Procesamiento de eventos fuera de orden**\n",
    "\n",
    "Apache Beam también admite el procesamiento de eventos fuera de orden. Esto significa que los eventos del stream pueden llegar en un orden diferente al que fueron generados. Apache Beam utiliza técnicas como **watermarking** para identificar el tiempo real de cada evento y procesarlo correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí te explico todo sobre Cloud Dataflow:\n",
    "\n",
    "**¿Qué es Cloud Dataflow?**\n",
    "\n",
    "* Es un servicio totalmente gestionado de Google Cloud Platform (GCP) diseñado para la ejecución de pipelines de datos escalables y de alto rendimiento.\n",
    "* Está basado en el modelo de programación y el framework de código abierto Apache Beam.\n",
    "* Esencialmente, Dataflow toma tus pipelines de Apache Beam y los ejecuta de manera optimizada en la infraestructura de Google Cloud.\n",
    "\n",
    "**Características clave**\n",
    "\n",
    "* **Procesamiento unificado:** Maneja tanto datos en lotes (batch) como en tiempo real (streaming) con una misma programación simplificada.\n",
    "* **Totalmente gestionado:** Reduce la carga operativa al encargarse del aprovisionamiento automático de recursos, escalado, monitoreo y tolerancia a fallas. No tienes que preocuparte por la administración de clusters. \n",
    "* **Escalabilidad sin esfuerzo:** Se adapta a las demandas de datos, aumentando o disminuyendo recursos de procesamiento de forma automática. Esto garantiza el rendimiento y te permite procesar desde pequeños flujos hasta cargas de trabajo masivas.\n",
    "* **Integración nativa con GCP:** Se conecta fácilmente con otros servicios de Google Cloud como BigQuery, Pub/Sub, Cloud Storage, etc., formando un ecosistema de procesamiento de datos eficiente.\n",
    "* **Rentable:** Modelo de precios basado en el uso, lo que significa que solo pagas por los recursos que efectivamente consumes.\n",
    "\n",
    "**¿Cuándo usar Cloud Dataflow?**\n",
    "\n",
    "* **Procesamiento complejo de datos:** Si tus necesidades involucran transformaciones de datos desafiantes a gran escala, Dataflow proporciona la potencia y escalabilidad requeridas.\n",
    "* **Agilidad ETL/ELT:** Ideales para ejecutar tareas de extracción, transformación y carga (ETL) o su variante de transformación primero (ELT) en la nube.\n",
    "* **Análisis de streaming:** Para procesar datos en tiempo real y obtener información valiosa al instante.\n",
    "* **Migración de cargas de trabajo locales:** Simplifica migrar tus pipelines de Apache Beam existentes desde entornos on-premise a la nube.\n",
    "\n",
    "**¿Cómo funciona?**\n",
    "\n",
    "1. **Desarrollo de pipelines:** Creas tus pipelines utilizando los SDK de Apache Beam (Java, Python, Go, entre otros).\n",
    "2. **Ejecución en Dataflow:** Envías tu pipeline al servicio Cloud Dataflow.\n",
    "3. **Aprovisionamiento de recursos:** Dataflow administra y aprovisiona de forma automática los recursos necesarios en Google Cloud (normalmente, instancias de Compute Engine).\n",
    "4. **Procesamiento distribuido:** Dataflow ejecuta tu pipeline de forma paralela y escalable en los recursos asignados.\n",
    "5. **Monitoreo y Optimización:** Puedes supervisar el funcionamiento del pipeline y Dataflow optimiza la ejecución del mismo según su desempeño.\n",
    "\n",
    "[https://cloud.google.com/dataflow](https://cloud.google.com/dataflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam ofrece una amplia gama de conectores para leer y escribir datos desde y hacia diversos sistemas, tanto de Google Cloud Platform (GCP) como de terceros. Estos conectores facilitan la integración de Apache Beam con diferentes fuentes y destinos de datos, lo que lo convierte en una herramienta versátil para el procesamiento de datos a gran escala.\n",
    "\n",
    "**Categorías principales de conectores:**\n",
    "\n",
    "1. **Conectores I/O:** Sirven para leer y escribir datos en diferentes formatos y ubicaciones, como archivos locales, bases de datos, sistemas de mensajería y servicios en la nube. Algunos ejemplos:\n",
    "    * **Leer desde texto:** `ReadFromText`, `ReadFromParquet`, `ReadFromPubSub`\n",
    "    * **Escribir en texto:** `WriteToText`, `WriteToParquet`, `WriteToPubSub`\n",
    "    * **Bases de datos:** `ReadFromJdbc`, `WriteToJdbc`\n",
    "    * **Almacenamiento en la nube:** `ReadFromBigtable`, `WriteToBigtable`, `ReadFromGcs`, `WriteToGcs`\n",
    "2. **Conectores de transformación:** Permiten aplicar transformaciones personalizadas a los datos durante el procesamiento. Entre ellos:\n",
    "    * **Combiners:** Combinan elementos de un PCollection en un valor único.\n",
    "    * **Windowing:** Agrupan elementos de un PCollection en ventanas temporales.\n",
    "    * **Coding:** Serializan y deserializan los datos para su transmisión y almacenamiento.\n",
    "3. **Conectores de monitoreo:** Facilitan el seguimiento del rendimiento y la ejecución de los pipelines de Apache Beam. Algunos ejemplos:\n",
    "    * **Cloud Monitoring:** Envía métricas al sistema de monitoreo de GCP.\n",
    "    * **Logback:** Registra eventos en archivos de registro.\n",
    "4. **Conectores de SDK:** Proporcionan interfaces para interactuar con SDKs de terceros y ampliar las capacidades de Apache Beam.\n",
    "\n",
    "**Conectores destacados:**\n",
    "\n",
    "* **Google Cloud Pub/Sub:** Para leer y escribir mensajes en un tema de Pub/Sub.\n",
    "* **Apache Kafka:** Para interactuar con un broker de Kafka y procesar streams de datos.\n",
    "* **Cloud Spanner:** Para leer y escribir datos en una base de datos Cloud Spanner.\n",
    "* **Cloud Bigtable:** Para leer y escribir datos en una base de datos NoSQL Cloud Bigtable.\n",
    "* **Cloud Datastore:** Para leer y escribir datos en una base de datos NoSQL Cloud Datastore.\n",
    "* **AWS S3:** Para leer y escribir datos en un bucket de Amazon S3.\n",
    "* **Hadoop HDFS:** Para leer y escribir datos en un sistema de archivos distribuido HDFS.\n",
    "\n",
    "**Nota:** La lista completa de conectores disponibles para Apache Beam es extensa y se actualiza constantemente. Puedes consultar la documentación oficial para ver la lista actualizada: [https://beam.apache.org/documentation/io/connectors/](https://beam.apache.org/documentation/io/connectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo simula un proceso ETL (Extracción, Transformación y Carga) utilizando Apache Beam en Python. Los datos se obtienen desde un stream de Kafka, se limpian, se agrega un promedio y se almacenan en buckets de Cloud Storage.\n",
    "\n",
    "**Pasos:**\n",
    "\n",
    "1. **Importaciones:**\n",
    "   Se importan las librerías necesarias para trabajar con Apache Beam, Kafka y Cloud Storage.\n",
    "\n",
    "```python\n",
    "import apache_beam as beam\n",
    "from apache_beam.io.kafka import KafkaIO\n",
    "from apache_beam.io.gcp.gcs import GCSFileSystem\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "from apache_beam.transforms.aggregation import CombinePerKey\n",
    "```\n",
    "\n",
    "2. **Definición de la ventana:**\n",
    "   Se establece la ventana de tiempo fija que se utilizará para agrupar los datos. En este caso, la ventana será de 1 minuto.\n",
    "\n",
    "```python\n",
    "window_size = 60  # Segundos\n",
    "ventana = FixedWindows(window_size)\n",
    "```\n",
    "\n",
    "3. **Lectura de datos desde Kafka:**\n",
    "   Se define un lector de Kafka para consumir los datos del stream. Se especifica el tema de Kafka y el formato de los mensajes (JSON).\n",
    "\n",
    "```python\n",
    "kafka_source = (\n",
    "    KafkaIO.read_from_topic(\"topic-entrada\")\n",
    "    | \"DecodificarMensaje\" >> beam.Map(lambda mensaje: json.loads(mensaje))\n",
    ")\n",
    "```\n",
    "\n",
    "4. **Limpieza de datos:**\n",
    "   Se elimina cualquier registro que contenga valores nulos en el campo \"valor\".\n",
    "\n",
    "```python\n",
    "datos_sin_nulos = (\n",
    "    kafka_source\n",
    "    | \"FiltrarValoresNulos\" >> beam.Filter(lambda dato: dato[\"valor\"] is not None)\n",
    ")\n",
    "```\n",
    "\n",
    "5. **Cálculo del promedio:**\n",
    "   Se agrupa la información por clave (\"sensor\") y se calcula el promedio del valor dentro de cada ventana.\n",
    "\n",
    "```python\n",
    "promedio_por_sensor = (\n",
    "    datos_sin_nulos\n",
    "    | \"Ventanas\" >> beam.WindowInto(ventana)\n",
    "    | \"CalcularPromedio\" >> beam.CombinePerKey(CombinePerKey(mean))\n",
    ")\n",
    "```\n",
    "\n",
    "6. **Formato de salida:**\n",
    "   Se transforma la información en un formato adecuado para guardarla en Cloud Storage (JSON).\n",
    "\n",
    "```python\n",
    "datos_formato_salida = (\n",
    "    promedio_por_sensor\n",
    "    | \"PrepararSalida\" >> beam.Map(lambda dato: json.dumps(dato))\n",
    ")\n",
    "```\n",
    "\n",
    "7. **Escritura en Cloud Storage:**\n",
    "   Se utiliza un escritor de Cloud Storage para almacenar los datos en buckets. Se especifica el bucket y el nombre del archivo.\n",
    "\n",
    "```python\n",
    "gcs_writer = GCSFileSystem.create(bucket=\"bucket-destino\")\n",
    "destino = gcs_writer.join(\"datos_procesados/fecha={}.json\".format(str(datetime.datetime.now())))\n",
    "\n",
    "datos_formato_salida | \"EscribirACloudStorage\" >> beam.io.WriteToFile(destino)\n",
    "```\n",
    "\n",
    "**Ejecución del pipeline:**\n",
    "\n",
    "Para ejecutar este ejemplo, necesitarás tener Apache Beam instalado y configurado para acceder a tu entorno de Kafka y Cloud Storage. Una vez hecho esto, puedes ejecutar el siguiente comando:\n",
    "\n",
    "```\n",
    "beam.Pipeline(options=PipelineOptions()).run(pipeline)\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "El código anterior define un pipeline de Apache Beam que realiza las siguientes tareas:\n",
    "\n",
    "1. **Lee datos desde un stream de Kafka:** El lector de Kafka consume los mensajes del tema especificado y los convierte en objetos Python.\n",
    "2. **Limpia los datos:** Se eliminan los registros que contienen valores nulos en el campo \"valor\".\n",
    "3. **Calcula el promedio:** Se agrupa la información por clave (\"sensor\") y se calcula el promedio del valor dentro de cada ventana de 1 minuto.\n",
    "4. **Formatea la salida:** Se transforma la información en un formato JSON para que sea fácil de almacenar.\n",
    "5. **Escribe en Cloud Storage:** Se escribe la información formateada en un archivo JSON en un bucket de Cloud Storage.\n",
    "\n",
    "Este es un ejemplo básico de un ETL con Apache Beam. Puedes modificarlo para adaptarlo a tus necesidades específicas, como cambiar la ventana de tiempo, agregar más transformaciones o usar otros conectores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2024.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
